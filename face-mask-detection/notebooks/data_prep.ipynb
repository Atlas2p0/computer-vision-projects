{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from natsort import natsorted\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/workspace/projects/vision/face-mask-detection/notebooks/../data/images'),\n",
       " PosixPath('/workspace/projects/vision/face-mask-detection/notebooks/../data/annotations'))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = Path(os.getcwd() + \"/../data/images\")\n",
    "annotations_dir = Path(os.getcwd()+\"/../data/annotations\")\n",
    "image_dir, annotations_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(xml_file):\n",
    "# Get xml tree root\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Get all \"ojbect\" tags in the file\n",
    "    objects = root.findall('object')\n",
    "    # Get annotations which contain all labels and boundboxes\n",
    "    object_annotations = []\n",
    "    for obj in objects:\n",
    "        # Get bound box coords and labels for each face in the image\n",
    "        label = obj.find('name').text\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "\n",
    "        object_annotations.append({\n",
    "            'label(s)': label,\n",
    "            'bbox(s)': [xmin, ymin, xmax, ymax]\n",
    "        })\n",
    "    return object_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a `torch.utils.data` Dataset and Visualizing Images, bboxes and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceMaskDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_dir, transform= None):\n",
    "        super().__init__()\n",
    "        # Getting a sorted list of all image and annotations file names\n",
    "        self.image_paths = natsorted(list(pathlib.Path(image_dir).glob(\"*.png\")))\n",
    "        self.annotation_paths = natsorted(list(pathlib.Path(annotations_dir).glob(\"*.xml\")))\n",
    "\n",
    "        # Getting transforms if found\n",
    "        self.transform = transform\n",
    "        # class_to_idx will be used when training a model\n",
    "        self.class_to_idx = {\"with_mask\": 0, \"without_mask\": 1, \"mask_weared_incorrect\": 2}\n",
    "        self.classes = [\"with_mask\", \"without_mask\", \"mask_weared_incorrect\"] \n",
    "\n",
    "    # Overriding the __getitem__() function to return a PIL image and its associated annotations\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        object_annotations = parse_xml(self.annotation_paths[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            original_width, original_height = image.size\n",
    "            image = self.transform(image)\n",
    "            new_width, new_height = image.shape[1], image.shape[2]\n",
    "            width_scale= new_width / original_width\n",
    "            height_scale= new_height / original_height\n",
    "            \n",
    "            for annotation in object_annotations:\n",
    "                xmin, ymin, xmax, ymax = annotation['bbox(s)']\n",
    "                annotation['bbox(s)'] = [\n",
    "                    int(xmin * width_scale),\n",
    "                    int(ymin * height_scale),\n",
    "                    int(xmax * width_scale),\n",
    "                    int(ymax * height_scale)\n",
    "                ]\n",
    "                label = annotation['label(s)']\n",
    "                annotation['label(s)'] = torch.tensor(self.class_to_idx[label], dtype= torch.int64)\n",
    "                annotation['bbox(s)'] = torch.tensor(annotation['bbox(s)'], dtype= torch.float32)\n",
    "\n",
    "\n",
    "            return image, object_annotations\n",
    "        else:\n",
    "            return image, object_annotations\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if len(self.image_paths) == len(self.annotation_paths): \n",
    "            return len(self.image_paths)\n",
    "        else:\n",
    "            print(\"Error num of images != num of annotations \\n\")\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size= (224, 224))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = FaceMaskDataset(image_dir, annotations_dir, transform= train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label(s)': tensor(0), 'bbox(s)': tensor([66., 45., 90., 80.])},\n",
       " {'label(s)': tensor(1), 'bbox(s)': tensor([203.,  25., 224.,  55.])},\n",
       " {'label(s)': tensor(1), 'bbox(s)': tensor([107.,  89., 128., 121.])},\n",
       " {'label(s)': tensor(0), 'bbox(s)': tensor([139.,  80., 162., 117.])}]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = all_data.__getitem__(5)\n",
    "ann_test = data[1]\n",
    "ann_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65051/298312334.py:9: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  np_image = np.array(tensor_image).copy()\n"
     ]
    }
   ],
   "source": [
    "def visualize_random_image_with_bbox(dataset: Dataset):\n",
    "    index = random.randrange(1, dataset.__len__() - 1)\n",
    "    data = dataset.__getitem__(index)\n",
    "    \n",
    "    # Permute tensor image for the preferred shape by opencv (C,H,W) -> (H,W,C)\n",
    "    tensor_image, image_annotations = data[0], data[1]\n",
    "    tensor_image = torch.permute(tensor_image, (1, 2, 0))\n",
    "    \n",
    "    np_image = np.array(tensor_image).copy()\n",
    "    opencv_image = cv2.cvtColor(np_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "\n",
    "    for ann in image_annotations:\n",
    "        label_int = ann['label(s)'].item()\n",
    "        label_txt = dataset.classes[label_int]\n",
    "        bbox = ann['bbox(s)']\n",
    "\n",
    "        xmin, ymin, xmax, ymax = int(bbox[0].item()), int(bbox[1].item()), int(bbox[2].item()), int(bbox[3].item())\n",
    "        cv2.rectangle(img= opencv_image,\n",
    "                      pt1= (xmin,ymin),\n",
    "                      pt2= (xmax,ymax),\n",
    "                      color= (0, 0, 255),\n",
    "                      shift= 0)\n",
    "        cv2.putText(img= opencv_image,\n",
    "                    text= label_txt,\n",
    "                    org= (xmin - 10, ymin - 5),\n",
    "                    fontFace= cv2.FONT_HERSHEY_PLAIN,\n",
    "                    fontScale= 1,\n",
    "                    color= (0, 255, 0),\n",
    "                    thickness= 2)\n",
    "    \n",
    "    cv2.imshow(\"Image\", opencv_image)\n",
    "    cv2.waitKey(6000)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "visualize_random_image_with_bbox(all_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682, 171)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = FaceMaskDataset(image_dir= image_dir, annotations_dir= annotations_dir, transform= train_transforms)\n",
    "\n",
    "train_dataset, test_dataset = train_test_split(dataset, train_size= 0.80, random_state= 42)\n",
    "train_dataset.__len__(), test_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]),\n",
       " [{'label(s)': tensor(0), 'bbox(s)': tensor([ 72.,  42., 147.,  76.])},\n",
       "  {'label(s)': tensor(1), 'bbox(s)': tensor([186.,  33., 224.,  65.])},\n",
       "  {'label(s)': tensor(1), 'bbox(s)': tensor([47., 53., 76., 68.])},\n",
       "  {'label(s)': tensor(0), 'bbox(s)': tensor([ 9., 39., 32., 51.])}])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size= 2,\n",
    "                              shuffle= True,\n",
    "                              num_workers= 0,\n",
    "                              collate_fn= custom_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size= 2,\n",
    "                             shuffle= False,\n",
    "                             num_workers= 0,\n",
    "                             collate_fn= custom_collate_fn)\n",
    "images, annotations = next(iter(train_dataloader))\n",
    "images[0].shape, annotations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
